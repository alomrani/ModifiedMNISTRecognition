{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST-Competition.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnbSF74vygze",
        "colab_type": "text"
      },
      "source": [
        "# **Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XQZ1K3eSlXR",
        "colab_type": "code",
        "outputId": "7bcf60aa-a72e-4048-8a37-bee79f710649",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import collections\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import pylab\n",
        "use_colab = True\n",
        "if use_colab:\n",
        "  from google.colab import drive\n",
        "import tensorflow as tf\n",
        "TINY = 1e-30\n",
        "EPS = 1e-4\n",
        "nax = np.newaxis\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "if use_colab:\n",
        "  drive_name = '/content/drive'\n",
        "  drive.mount(drive_name)\n",
        "  drive_413_A1_folder = 'Queens_data_task'\n",
        "  drive_location = drive_name + '/My Drive/' + drive_413_A1_folder  # Change this to where your files are located\n",
        "else:\n",
        "  # set the drive_location variable to whereever the extracted contents are.\n",
        "  drive_location = ''\n",
        "\n",
        "# Hyper-parameters \n",
        "input_size = 4096   # 784\n",
        "num_classes = 10\n",
        "num_epochs = 100\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "# MNIST dataset (images and labels)\n",
        "Y = np.loadtxt(drive_location + '/' +\"train_y.csv\", delimiter=\",\")\n",
        "Y = Y.reshape(-1)\n",
        "Y = np.int_(Y)\n",
        "X = np.load(drive_location + '/' + 'X.npy')\n",
        "X = torch.Tensor(X)\n",
        "Y = torch.tensor(Y, dtype=torch.int64)\n",
        "X = X.reshape(-1, input_size)\n",
        "# Logistic regression model\n",
        "model = nn.Linear(input_size, num_classes)\n",
        "\n",
        "# Loss and optimizer\n",
        "# nn.CrossEntropyLoss() computes softmax internally\n",
        "criterion = nn.CrossEntropyLoss()  \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
        "\n",
        "# Train the model\n",
        "#total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    for i in range(50000//batch_size):\n",
        "        # Reshape images to (batch_size, input_size)\n",
        "        tempX = X[i*batch_size: i*batch_size + batch_size]\n",
        "        tempY = Y[i*batch_size: i*batch_size + batch_size]\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(tempX)\n",
        "        loss = criterion(outputs, tempY)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "        \n",
        "    \n",
        "\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    images = X.reshape(-1, input_size)\n",
        "    outputs = model(X)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    correct += (predicted == Y).sum()\n",
        "\n",
        "    print('Accuracy of the model on the 50000 test images: {} %'.format(100 * correct / 50000))\n",
        "\n",
        "# Save the model checkpoint\n",
        "torch.save(model.state_dict(), 'model.ckpt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "Accuracy of the model on the 10000 test images: 17 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3M2Rg4Lyypq",
        "colab_type": "text"
      },
      "source": [
        "As we can see, the logistic regression model fails to classify the images and has a test accuracy of 17 % only. This is because detecting the biggest digit in the image is a highly non-linear problem. Logistic regression models have (piecewise) linear decision boundaries so they are not complex enough to learn the features in the training images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwNFwOtoznwD",
        "colab_type": "text"
      },
      "source": [
        "#  **Convolutional neural networks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXFjgIiymU4z",
        "colab_type": "code",
        "outputId": "5a6530a3-513a-4d0a-8cb7-c71f1238b4cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import collections\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import pylab\n",
        "use_colab = True\n",
        "if use_colab:\n",
        "  from google.colab import drive\n",
        "import tensorflow as tf\n",
        "TINY = 1e-30\n",
        "EPS = 1e-4\n",
        "nax = np.newaxis\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "if use_colab:\n",
        "  drive_name = '/content/drive'\n",
        "  drive.mount(drive_name)\n",
        "  drive_413_A1_folder = 'Queens_data_task'\n",
        "  drive_location = drive_name + '/My Drive/' + drive_413_A1_folder  # Change this to where your files are located\n",
        "else:\n",
        "  # set the drive_location variable to whereever the extracted contents are.\n",
        "  drive_location = ''\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xdk8nTamwrr_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import random\n",
        "# Hyper-parameters \n",
        "input_size = 4096   # 784\n",
        "num_classes = 10\n",
        "num_epochs = 35\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "momentum = 0.8\n",
        "\n",
        "class Net(nn.Module):    \n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "          \n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "          \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p = 0.5),\n",
        "            nn.Linear(64 * 16 * 16, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p = 0.5),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p = 0.5),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "          \n",
        "        for m in self.features.children():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "        \n",
        "        for m in self.classifier.children():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform(m.weight)\n",
        "            elif isinstance(m, nn.BatchNorm1d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "                \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        \n",
        "        return x     \n",
        "\n",
        "\n",
        "# MNIST dataset (images and labels)\n",
        "Y = np.loadtxt(drive_location + '/' +\"train_y.csv\", delimiter=\",\")\n",
        "Y = Y.reshape(-1)\n",
        "Y = np.int_(Y)\n",
        "X = np.load(drive_location + '/' + 'X.npy')\n",
        "X = torch.Tensor(X).reshape(-1,1,64,64)\n",
        "Y = torch.tensor(Y, dtype=torch.int64).type(torch.cuda.FloatTensor)\n",
        "X = X.reshape(-1, input_size).type(torch.cuda.FloatTensor)\n",
        "\n",
        "network = Net()\n",
        "network.cuda()\n",
        "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
        "                      momentum=momentum)\n",
        "# Train the model\n",
        "#total_step = len(train_loader)\n",
        "network.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(25000//batch_size):\n",
        "        # Reshape images to (batch_size, input_size)\n",
        "        tempX = X[i*batch_size: i*batch_size + batch_size].reshape(-1, 1,64,64)\n",
        "        tempY = Y[i*batch_size: i*batch_size + batch_size].type(torch.int64)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = network(tempX)\n",
        "        loss = nn.CrossEntropyLoss()(output, tempY)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "        \n",
        "    \n",
        "        print ('Epoch [{}/{}], Loss: {:.4f}' \n",
        "              .format(epoch+1, num_epochs, loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyTc2Zgj0k82",
        "colab_type": "code",
        "outputId": "662db07e-11b5-4e37-af11-0fbc59b2be76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i in range(0,25): #since GPU does not have enough memory\n",
        "      images = X[25000 + 1000*i:25000 + 1000*i + 1000].reshape(-1,1,64,64)\n",
        "      outputs = network(images)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      correct += (predicted == Y[25000 + 1000*i:25000 + 1000*i + 1000]).sum()\n",
        "\n",
        "    print('Accuracy of the model on the 25000 test images: {} %'.format(100 * correct / 25000))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the model on the 25000 test images: 77 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSuVV9vbzyTq",
        "colab_type": "text"
      },
      "source": [
        "Our CNN consists of 4 convolution layers, each separated with a non-linearity (ReLU) in addition to Max pooling and Batch normalization to speed up training. The output of the CNN is then passed on to classification is which basically a simple feed forward NN. As we can see, the CNN performs much better on this task. This because we have introduced non-linearity to our network across multiple layers (unlike LR), in addition to convolution and Max pooling which allow us to efficiently learn low- and high-level features in the image.\n",
        "We were able to achieve a test accuracy of 77%, a higher accuracy can be achieved if we tune the hyper-parameters even further.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvrk2EBO9e1x",
        "colab_type": "text"
      },
      "source": [
        "# **RNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26MhpLior3I5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# Hyper-parameters \n",
        "# Hyper-parameters\n",
        "sequence_length = 64\n",
        "input_size = 64\n",
        "hidden_size = 164\n",
        "num_layers = 2\n",
        "num_classes = 10\n",
        "batch_size = 2500\n",
        "num_epochs = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "# MNIST dataset (images and labels)\n",
        "Y = np.loadtxt(drive_location + '/' +\"train_y.csv\", delimiter=\",\")\n",
        "Y = Y.reshape(-1)\n",
        "Y = np.int_(Y)\n",
        "X = np.load(drive_location + '/' + 'X.npy')\n",
        "X = torch.Tensor(X).reshape(-1,1,64,64)\n",
        "Y = torch.tensor(Y, dtype=torch.int64).type(torch.cuda.FloatTensor)\n",
        "X = X.reshape(-1, 4096).type(torch.cuda.FloatTensor)\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, nonlinearity='relu')\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Set initial hidden and cell states \n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
        "        \n",
        "        # Forward propagate LSTM\n",
        "        out, _ = self.lstm(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
        "        \n",
        "        # Decode the hidden state of the last time step\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "\n",
        "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "#model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(25000//batch_size):\n",
        "        # Reshape images to (batch_size, input_size)\n",
        "        tempX = X[i*batch_size: i*batch_size + batch_size].reshape(-1,64,64)\n",
        "        tempY = Y[i*batch_size: i*batch_size + batch_size].type(torch.int64)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model(tempX)\n",
        "        loss = criterion(output, tempY)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "        \n",
        "    \n",
        "        print ('Epoch [{}/{}], Loss: {:.4f}' \n",
        "              .format(epoch+1, num_epochs, loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71MiHZNF6b0l",
        "colab_type": "code",
        "outputId": "22acaa2a-5f1b-4a57-a3e1-37f026136f40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i in range(0,25): #since GPU does not have enough memory\n",
        "      images = X[25000 + 1000*i:25000 + 1000*i + 1000].reshape(-1,64,64)\n",
        "      outputs = model(images)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      correct += (predicted == Y[25000 + 1000*i:25000 + 1000*i + 1000]).sum()\n",
        "\n",
        "    print('Accuracy of the model on the 25000 test images: {} %'.format(100 * correct / 25000))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the model on the 25000 test images: 10 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMBSqUtA1iyR",
        "colab_type": "text"
      },
      "source": [
        "While RNNs do have non-linearities which enable them to learn complex features, they are mainly designed for tasks that have variable length inputs like speeach recognition or machine translation. This classification task requires recognizing patterns across space which CNNs can do very well in. RNNs, however, are designed to build up memory so they can learn from previous inputs. In other words, RNNs can perform well in problems that involve temporal locality which CNNs do well in tasks with spatial locality. \n",
        "\n",
        "It is clear that using an RNN for this task is no better that randomly guessing even after training for 100 epochs with batch size 2500!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d_Gv6ud9qSQ",
        "colab_type": "text"
      },
      "source": [
        "# **Gated CNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rZXnrStAHQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# Hyper-parameters \n",
        "input_size = 64\n",
        "hidden_size = 164\n",
        "num_channels = 15\n",
        "num_classes = 10\n",
        "batch_size = 100\n",
        "num_epochs = 100\n",
        "learning_rate = 0.01\n",
        "\n",
        "# MNIST dataset (images and labels)\n",
        "Y = np.loadtxt(drive_location + '/' +\"train_y.csv\", delimiter=\",\")\n",
        "Y = Y.reshape(-1)\n",
        "Y = np.int_(Y)\n",
        "X = np.load(drive_location + '/' + 'X.npy')\n",
        "X = torch.Tensor(X).reshape(-1,1,64,64)\n",
        "Y = torch.tensor(Y, dtype=torch.int64).type(torch.cuda.FloatTensor)\n",
        "X = X.reshape(-1, 4096).type(torch.cuda.FloatTensor)\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "class GLUblock(nn.Module):\n",
        "    def __init__(self, k, in_c, out_c):\n",
        "        super(GLUblock, self).__init__()\n",
        "        #only need to change shape of the residual if num_channels changes (i.e. in_c != out_c)\n",
        "        if in_c == out_c:\n",
        "            self.use_proj=0\n",
        "        else:\n",
        "            self.use_proj=1\n",
        "        self.convresid=nn.utils.weight_norm(nn.Conv2d(in_c, out_c, kernel_size=(1,1)),name='weight')\n",
        "        \n",
        "        \n",
        "        self.convx1b = nn.utils.weight_norm(nn.Conv2d(in_c, out_c, kernel_size=k,padding=k//2),name='weight')\n",
        "        self.convx2b = nn.utils.weight_norm(nn.Conv2d(in_c, out_c, kernel_size=k,padding=k//2),name='weight')\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        if self.use_proj==1:# if in_c != out_c, need to change size of residual\n",
        "            residual=self.convresid(residual)\n",
        "        \n",
        "        x1 = self.convx1b(x) \n",
        "        x2 = self.convx2b(x) \n",
        "        x2 = torch.sigmoid(x2)\n",
        "        x=torch.mul(x1,x2) \n",
        "        return x+residual\n",
        "\n",
        "class GCNN(nn.Module):\n",
        "  def __init__(self, k, in_c, out_c):\n",
        "    super(GCNN, self).__init__()\n",
        "    self.GCN_layers = nn.Sequential(\n",
        "        GLUblock(k, in_c, out_c),\n",
        "        nn.MaxPool2d(kernel_size=2),\n",
        "        nn.BatchNorm2d(out_c),\n",
        "        GLUblock(k, out_c, out_c),\n",
        "        nn.MaxPool2d(kernel_size=2),\n",
        "        nn.BatchNorm2d(out_c),\n",
        "        GLUblock(k, out_c, out_c)\n",
        "    )\n",
        "    self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p = 0.5),\n",
        "            nn.Linear(16 * 16 * out_c, 16*16),\n",
        "            nn.BatchNorm1d(16 * 16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p = 0.5),\n",
        "            nn.Linear(16 * 16, 50),\n",
        "            nn.BatchNorm1d(50),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p = 0.5),\n",
        "            nn.Linear(50, 10),\n",
        "        )\n",
        "  def forward(self, x):\n",
        "    out = self.GCN_layers(x)\n",
        "    out = out.view(out.size(0), -1)\n",
        "    out = self.classifier(out)\n",
        "    return out\n",
        "\n",
        "\n",
        "model = GCNN(5, 1, num_channels).to(device)\n",
        "\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "#model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(25000//batch_size):\n",
        "        # Reshape images to (batch_size, input_size)\n",
        "        tempX = X[i*batch_size: i*batch_size + batch_size].reshape(-1,1,64,64)\n",
        "        tempY = Y[i*batch_size: i*batch_size + batch_size].type(torch.int64)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model(tempX)\n",
        "        loss = criterion(output, tempY)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "        \n",
        "    \n",
        "        print ('Epoch [{}/{}], Loss: {:.4f}' \n",
        "              .format(epoch+1, num_epochs, loss.item()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFlutLU4hkLQ",
        "colab_type": "code",
        "outputId": "f7eece30-8e51-4e7d-bdc8-5b9be335fa19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i in range(25): #since GPU does not have enough memory\n",
        "      images = X[25000 + 1000*i:25000 + 1000*i + 1000].reshape(-1,1,64,64)\n",
        "      outputs = model(images)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      correct += (predicted == Y[25000 + 1000*i:25000 + 1000*i + 1000]).sum()\n",
        "\n",
        "    print('Accuracy of the model on the 25000 test images: {} %'.format(100 * correct / 25000))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the model on the 25000 test images: 80 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1k679YxW3sfn",
        "colab_type": "text"
      },
      "source": [
        "As we can see, gated CNNs even perform better on this task by achieving a test accuracy of 80 % (further training and tuning of hyper-parameters can yield higher accuracy). Gated linear units are a simple gating mechanism designed to tackle the vanishing gradient problem by having linear units coupled with gates. This retains the non-linear capabilities of the layers while allowing the gradient to propagate through the linear units. In other words, this technique allows some features to propagate through some layers, preventing the low-level features from vanishing deep into the network."
      ]
    }
  ]
}